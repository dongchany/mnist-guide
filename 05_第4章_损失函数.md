# 第 4 章：衡量好坏——损失函数

> **本章目标**：理解交叉熵损失函数的原理，知道为什么不能直接用"准确率"来训练。
> **预计时间**：1.5 小时

---

## 4.1 直觉：什么是"好"的预测？

模型对一张数字"3"的图片给出两种预测：

| | 类别0 | 类别1 | 类别2 | 类别3 | ... |
|---|---|---|---|---|---|
| **预测 A** | 0.01 | 0.01 | 0.01 | **0.90** | ... |
| **预测 B** | 0.10 | 0.10 | 0.10 | **0.30** | ... |

两个预测都会选类别 3（概率最高），准确率都是"猜对了"。但直觉上 A 比 B **好得多**——A 很有信心，B 几乎是在瞎蒙。

我们需要一个**损失函数**来区分这种差异。损失函数给每个预测打一个"差劲程度"的分数——越低越好。

---

## 4.2 为什么不能直接优化准确率？

准确率 = 猜对的数量 / 总数量。听起来最直接，为什么不用？

问题在于：准确率对权重的微小变化**没有梯度**。

想象你稍微调整了一个权重。如果这个调整没有改变"谁是最大概率"，那准确率不变——梯度为 0。只有在刚好跨过决策边界时才突变。这种"阶梯形"函数无法用来做梯度下降。

我们需要一个**平滑的**函数，即使预测只稍微好了一点点，损失值也能反映出来。

---

## 4.3 交叉熵损失

**交叉熵**（Cross-Entropy）是分类问题中最常用的损失函数。

### 单个样本

假设真实标签是 k（比如 k=3），模型的 Softmax 输出给类别 k 的概率是 pₖ。

```
Loss = -log(pₖ)
```

就这么简单！我们只关心**正确类别被赋予了多大的概率**。

**性质**：

- 如果 pₖ = 1（完美预测）→ Loss = -log(1) = 0
- 如果 pₖ = 0.5 → Loss = -log(0.5) = 0.693
- 如果 pₖ = 0.01 → Loss = -log(0.01) = 4.605
- 如果 pₖ → 0 → Loss → ∞

概率越小，惩罚越大，而且是**指数级**增大。这逼迫模型给正确答案分配高概率。

### 批量样本

对 N 个样本取平均：

```math
Loss = -(1/N) Σᵢ log(p_yᵢ)
```

其中 yᵢ 是第 i 个样本的真实标签，p_yᵢ 是模型给真实标签的概率。

### 用 one-hot 编码的写法

如果把真实标签写成 one-hot 向量 **y** = [0, 0, 0, 1, 0, ..., 0]（只有正确类别位置为 1），那么：

```math
\mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{C} y_{ij} \cdot \log(p_{ij})
```

因为 $yᵢⱼ$ 只在正确类别处为 1，所以这个双重求和自动退化为只看正确类别——和上面的公式一样。

---

## 4.4 代码实现

```python
def cross_entropy_loss(probs, y_onehot):
    """
    Cross-entropy loss.
    
    probs: (N, K) — softmax output probabilities
    y_onehot: (N, K) — one-hot encoded true labels
    
    Returns: scalar loss value
    """
    # Clip to avoid log(0)
    clipped = np.clip(probs, 1e-12, 1.0)
    return -np.mean(np.sum(y_onehot * np.log(clipped), axis=1))
```

**验证**：

```python
# Perfect prediction
probs_good = np.array([[0.01, 0.01, 0.01, 0.97]])
y_true = np.array([[0, 0, 0, 1]])
print(cross_entropy_loss(probs_good, y_true))    # ≈ 0.030 (low, good!)

# Bad prediction
probs_bad = np.array([[0.25, 0.25, 0.25, 0.25]])
print(cross_entropy_loss(probs_bad, y_true))     # ≈ 1.386 (high, bad!)
```

---

## 4.5 交叉熵 + Softmax 的优美简化

这是本章最重要的公式。当损失函数是交叉熵、输出层是 Softmax 时，输出层的梯度有一个非常简洁的形式：

```math
\frac{\partial \mathcal{L}}{\partial Z_3} = \frac{1}{N} (A_3 - Y)
```

其中：

- **Z₃** — 第 3 层（输出层）的线性输出，即 $Z_3 = W_3 A_2 + b_3 $
- **A₃** — Z₃ 经过 Softmax 后的预测概率
- **Y** — 真实标签（one-hot 编码）
- **N** — 样本数

**这意味着**：预测概率减去真实标签，就是梯度。偏差越大，梯度越大，调整越剧烈。完美预测时梯度为零。

推导过程涉及 Softmax 的求导（有兴趣可以查阅附录），但在实际编码中，我们只需要记住这个结论：

```python
# Output layer gradient — elegantly simple
dZ3 = (softmax_output - one_hot_labels) / N
```

---

## 4.6 损失的"地形"

可以把损失函数想象成一个巨大的"山丘地形"——52,650 维的空间（每个权重是一个维度），每个点的高度就是损失值。

我们的目标：**找到地形中的低点**（低损失 = 好预测）。

怎么找？往"下坡方向"走一步，再看看方向，再走一步……这就是梯度下降，下一章的主题。

---

## 4.7 动手练习

**练习 1**：手动计算——真实标签是类别 2，模型输出概率 [0.1, 0.2, 0.6, 0.1]。交叉熵损失是多少？

**练习 2**：画一个图，x 轴是 p（正确类别的概率，从 0.01 到 1.0），y 轴是 -log(p)。观察函数的形状。

**练习 3**：思考题——为什么要加 `np.clip(probs, 1e-12, 1.0)`？如果不加会怎样？
