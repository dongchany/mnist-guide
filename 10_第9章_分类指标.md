# 第 9 章：你的模型有多好？分类指标全解

> **本章目标**：理解准确率之外的多种评估指标，知道何时用哪个。
> **预计时间**：2 小时

---

## 9.1 为什么准确率不够？

假设一个数据集中 90% 是数字"0"，10% 是其他数字。一个什么都不做、永远猜"0"的模型准确率就有 90%——但它其实什么都没学到。

这就是为什么我们需要多种指标，从不同角度评估模型。

---

## 9.2 混淆矩阵：一切指标的基础

混淆矩阵 C 是一个 K×K 的表格（K 是类别数），其中 C[i][j] = 真实标签为 i 但被预测为 j 的样本数。

```
              Predicted
              0  1  2
Actual  0  [ 90  5  5 ]    ← 100 个真实的 0，90 个预测对了
        1  [  3 85 12 ]    ← 100 个真实的 1，85 个预测对了
        2  [  2  8 90 ]    ← 100 个真实的 2，90 个预测对了
```

- **对角线**：预测正确的数量
- **非对角线**：错误的类型和数量

```python
def confusion_matrix(y_true, y_pred, num_classes=10):
    cm = np.zeros((num_classes, num_classes), dtype=int)
    for t, p in zip(y_true, y_pred):
        cm[t, p] += 1
    return cm
```

---

## 9.3 各项指标详解

### 1. 准确率（Accuracy）

```
Accuracy = 正确预测数 / 总样本数 = Trace(CM) / Sum(CM)
```

最简单直观，但在类别不平衡时会误导。

### 2. 平衡准确率（Balanced Accuracy）

```
Balanced Accuracy = (1/K) Σᵢ Recall_i
```

每个类别的召回率取平均。即使某类样本很少，它的权重也和大类一样，避免了被大类"淹没"。

### 3. 精确率、召回率、F1（每类）

对于类别 i：

```
Precision_i = TP_i / (TP_i + FP_i)    "预测为 i 的里面，有多少真的是 i？"
Recall_i    = TP_i / (TP_i + FN_i)    "真实为 i 的里面，有多少被找到了？"
F1_i        = 2 · Prec · Rec / (Prec + Rec)    两者的调和均值
```

其中 TP_i = CM[i,i]，FP_i = 第 i 列之和 - TP_i，FN_i = 第 i 行之和 - TP_i。

### 4. Macro F1 vs Micro F1

```
Macro F1 = 各类 F1 的均值                    ← 对每个类别一视同仁
Micro F1 = 全局 TP/(全局 TP+FP+FN)          ← 在单标签分类中等于准确率
```

### 5. 交叉熵（Cross-Entropy）

```
CE = -(1/N) Σᵢ log(p_correct_class_i)
```

不仅看对错，还看模型的**信心程度**。即使两个模型准确率相同，交叉熵低的那个更好（预测更确定）。

### 6. 马修斯相关系数（MCC）

```
MCC = (c·s - Σₖ pₖ·tₖ) / √((s²-Σₖ pₖ²)(s²-Σₖ tₖ²))
```

其中 c = 对角线之和，s = 总样本数，pₖ = 第 k 列之和，tₖ = 第 k 行之和。

MCC 的范围是 [-1, 1]，其中 1 = 完美，0 = 随机，-1 = 完全相反。它被认为是**最可靠的单一指标**，因为它同时考虑了所有四种预测情况（TP, TN, FP, FN）。

### 7. Cohen's Kappa

```
Kappa = (p_o - p_e) / (1 - p_e)
```

p_o = 观测一致率（即准确率），p_e = 随机一致率。Kappa 衡量的是"比随机猜好了多少"。

---

## 9.4 高效实现：算一次混淆矩阵，推导全部指标

```python
def compute_all_metrics(y_true, y_pred, y_prob=None, num_classes=10):
    cm = confusion_matrix(y_true, y_pred, num_classes)
    n = cm.sum()
    c = np.trace(cm)                              # correct count
    p_col = cm.sum(axis=0)                         # predicted per class
    t_row = cm.sum(axis=1)                         # true per class
    
    # Per-class precision, recall, F1
    prec, rec, f1 = np.zeros(num_classes), np.zeros(num_classes), np.zeros(num_classes)
    for i in range(num_classes):
        tp = cm[i, i]
        fp, fn = p_col[i] - tp, t_row[i] - tp
        prec[i] = tp / (tp + fp) if tp + fp > 0 else 0
        rec[i]  = tp / (tp + fn) if tp + fn > 0 else 0
        f1[i]   = 2*prec[i]*rec[i] / (prec[i]+rec[i]) if prec[i]+rec[i] > 0 else 0
    
    # MCC
    mcc_num = c * n - np.dot(p_col, t_row)
    mcc_den = np.sqrt((n**2 - np.dot(p_col, p_col)) * (n**2 - np.dot(t_row, t_row)))
    
    # Kappa
    p_o = c / n
    p_e = np.dot(p_col, t_row) / (n * n)
    
    return {
        'accuracy': c / n,
        'balanced_accuracy': np.mean(rec),
        'macro_f1': np.mean(f1),
        'micro_f1': c / n,
        'mcc': mcc_num / mcc_den if mcc_den > 0 else 0,
        'cohen_kappa': (p_o - p_e) / (1 - p_e) if (1 - p_e) > 0 else 0,
        'confusion_matrix': cm,
    }
```

注意：所有指标都从同一个混淆矩阵推导出来，只需要算一次。

---

## 9.5 实际数据的分析

```python
metrics = compute_all_metrics(y_test, predictions)

print(f"Accuracy:     {metrics['accuracy']:.4f}")
print(f"Balanced Acc: {metrics['balanced_accuracy']:.4f}")
print(f"Macro F1:     {metrics['macro_f1']:.4f}")
print(f"MCC:          {metrics['mcc']:.4f}")
print(f"Kappa:        {metrics['cohen_kappa']:.4f}")
```

当所有指标都很高（>0.95）时，你可以比较确信模型是好的。如果某些指标之间有差距（比如准确率高但 Macro F1 低），说明某些类别表现不好，值得深入检查。

---

## 9.6 动手练习

**练习 1**：构造一个极端例子——90% 是类别 0，10% 是类别 1。模型永远猜 0。计算准确率、平衡准确率、MCC。体会它们的差异。

**练习 2**：在你训练的模型上计算所有指标。哪个数字最容易被认错？混淆矩阵能告诉你它经常被误认为什么。

**练习 3**：安装 sklearn，用 `sklearn.metrics` 验证你的指标实现是否正确。
