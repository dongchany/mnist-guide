# 第 2 章：单个神经元

> **本章目标**：理解神经元的三步操作——线性变换、激活、输出。
> **预计时间**：1.5 小时

---

## 2.1 直觉：一个"投票机器"

想象你要判断一张图片是不是数字"1"。你可能会关注：
- 中间那一列像素是不是很亮？（权重高）
- 左下角是不是空的？（权重低）

一个神经元做的事情就像一个**加权投票器**：

```
每个输入 × 对应的权重 → 求和 → 加偏置 → 通过激活函数 → 输出
```

---

## 2.2 数学：线性变换

给定输入向量 **x** = [x₁, x₂, ..., xₙ]（比如 784 个像素值），和权重向量 **w** = [w₁, w₂, ..., wₙ]，一个神经元计算：

```
z = w₁·x₁ + w₂·x₂ + ... + wₙ·xₙ + b
  = wᵀx + b
```

其中 b 是**偏置**（bias），可以理解为神经元的"默认倾向"。

**直觉**：每个权重 wᵢ 表示"第 i 个输入有多重要"。正权重表示"这个输入越大，越倾向于激活"；负权重表示"这个输入越大，越倾向于不激活"。

用 NumPy 写：

```python
def linear(x, w, b):
    """
    x: input vector, shape (n,)
    w: weight vector, shape (n,)
    b: bias scalar
    """
    return np.dot(x, w) + b
```

---

## 2.3 激活函数：给神经元加"开关"

如果只有线性变换，多层叠加还是线性的（线性的线性还是线性）。为了让网络能学习复杂的规律，我们需要**非线性**——激活函数。

### ReLU（修正线性单元）

最流行的激活函数，简单得令人惊讶：

```
ReLU(z) = max(0, z)
```

- 如果 z > 0，输出 z（原样通过）
- 如果 z ≤ 0，输出 0（关掉）

**直觉**：ReLU 就是一个开关。正信号通过，负信号阻断。

```python
def relu(z):
    """ReLU activation: pass positive, block negative."""
    return np.maximum(0, z)

# Example
z = np.array([-3, -1, 0, 2, 5])
print(relu(z))    # [0, 0, 0, 2, 5]
```

### ReLU 的导数

后面做反向传播时需要用到：

```
ReLU'(z) = 1  if z > 0
           0  if z ≤ 0
```

```python
def relu_derivative(z):
    """Gradient of ReLU: 1 for positive, 0 for non-positive."""
    return (z > 0).astype(z.dtype)
```

### 其他激活函数（了解即可）

**Sigmoid**：σ(z) = 1/(1 + e⁻ᶻ)，输出在 (0, 1) 之间。早期常用，但有"梯度消失"问题。

**Tanh**：tanh(z)，输出在 (-1, 1) 之间。比 Sigmoid 好一些。

**我们的选择**：隐藏层用 ReLU（简单、有效、训练快），输出层用 Softmax（下一章讲）。

---

## 2.4 组合起来：一个完整的神经元

```
输入 x → 线性变换 z = wᵀx + b → 激活 a = ReLU(z) → 输出
```

```python
def neuron(x, w, b):
    """A single neuron: linear transform + ReLU activation."""
    z = np.dot(x, w) + b      # linear
    a = relu(z)                 # activation
    return a
```

**一个神经元能做什么？** 它能学习一条"分界线"。比如在二维空间中，一个神经元可以学到"如果 2x₁ + 3x₂ - 1 > 0，就激活"。这条线把空间分成两半。

**一个神经元不能做什么？** 它只能做线性分割。对于我们的 10 类分类问题，一个神经元远远不够。我们需要把很多神经元**组合**起来——这就是下一章的内容。

---

## 2.5 从单个到批量：向量化

实际中我们不会一个样本一个样本地算。如果有 N 个样本，每个有 D 个特征：

```
X: (N, D) — N 个输入，每行一个样本
W: (D, M) — D 个输入特征到 M 个输出神经元
b: (1, M) — M 个偏置

Z = X @ W + b      # shape: (N, M)
A = relu(Z)         # shape: (N, M)
```

**直觉**：一次矩阵乘法 = 同时计算所有样本通过所有神经元的结果。这就是 NumPy 的威力。

```python
# 100 samples, 784 features → 64 neurons
X = np.random.randn(100, 784)
W = np.random.randn(784, 64)
b = np.zeros((1, 64))

Z = X @ W + b        # (100, 64) — each sample gets 64 outputs
A = np.maximum(0, Z)  # (100, 64) — ReLU applied element-wise
```

---

## 2.6 权重初始化：He 初始化

权重不能都初始化为 0（所有神经元会完全一样，无法学习），也不能太大（信号会爆炸）或太小（信号会消失）。

**He 初始化**适用于 ReLU 激活函数：

```python
def he_init(fan_in, fan_out):
    """He initialization: good for ReLU layers."""
    return np.random.randn(fan_in, fan_out) * np.sqrt(2.0 / fan_in)
```

**原理**：让每一层的输出方差大致保持在 1 左右，这样信号既不会爆炸也不会消失。

---

## 2.7 动手练习

**练习 1**：手动计算——给定 x = [1, 2, 3]，w = [0.5, -1, 0.3]，b = 0.1。计算 z 和 ReLU(z)。

**练习 2**：用 NumPy 创建 64 个神经元（输入 784 维），用 He 初始化权重。传入一个 shape (10, 784) 的随机输入，验证输出 shape 为 (10, 64)。

**练习 3**：思考题——如果把 ReLU 换成"什么都不做"（即 a = z），两层网络等价于什么？（提示：(X @ W₁ + b₁) @ W₂ + b₂ = X @ (W₁W₂) + ...）
