# 第 3 章：搭积木——多层网络与前向传播

> **本章目标**：理解多层网络结构、Softmax 输出层，实现完整的前向传播。
> **预计时间**：2 小时

---

## 3.1 直觉：为什么要多层？

一个神经元只能画一条线（线性边界）。但数字识别需要识别曲线、转角、交叉……

解决方案：**把很多神经元叠成多层**。

- **第一层**：学习简单的特征（"这里有条横线"，"这里有个角"）
- **第二层**：把第一层的简单特征组合成复杂特征（"左上有个圈+下面有竖线 → 可能是 9"）
- **输出层**：把所有特征汇总，做出最终判断

这就像拼乐高——简单的积木组合出复杂的结构。

---

## 3.2 网络结构

我们的全连接网络结构：

```
输入层        隐藏层 1      隐藏层 2      输出层
784 个像素 → 64 个神经元 → 32 个神经元 → 10 个输出
              ReLU           ReLU         Softmax
```

**全连接**的意思是：每一层的每个神经元都和上一层的所有神经元相连。

**参数量**计算：

- 第 1 层：784 × 64 + 64 = 50,240（权重 + 偏置）
- 第 2 层：64 × 32 + 32 = 2,080
- 第 3 层：32 × 10 + 10 = 330
- **总计**：52,650 个可学习参数

这些就是网络需要通过训练来"调整"的数字。

---

## 3.3 前向传播

数据从左到右"流过"网络的过程叫**前向传播**（Forward Propagation）。

### 第 1 层（输入 → 隐藏层 1）

```
Z₁ = X @ W₁ + b₁        # 线性变换, (N, 784) @ (784, 64) → (N, 64)
A₁ = ReLU(Z₁)            # 激活, (N, 64)
```

### 第 2 层（隐藏层 1 → 隐藏层 2）

```
Z₂ = A₁ @ W₂ + b₂       # (N, 64) @ (64, 32) → (N, 32)
A₂ = ReLU(Z₂)            # (N, 32)
```

### 第 3 层（隐藏层 2 → 输出层）

```
Z₃ = A₂ @ W₃ + b₃       # (N, 32) @ (32, 10) → (N, 10)
A₃ = Softmax(Z₃)         # (N, 10) — 每行是 10 个类别的概率
```

最后一层不用 ReLU，而用 Softmax——这是什么？

---

## 3.4 Softmax：把分数变成概率

输出层的 Z₃ 是一组"原始分数"（logits），可能是任意数值。我们需要把它们转换成概率。

**Softmax 函数**：

```math
Softmax(zᵢ) = e^zᵢ / Σⱼ e^zⱼ
```

**性质**：
- 所有输出都 > 0（因为指数函数恒正）
- 所有输出之和 = 1（归一化了）
- 最大的输入对应最大的概率

**例子**：

```
原始分数: z = [2.0, 1.0, 0.1]

e^z = [7.39, 2.72, 1.11]
sum = 11.22

Softmax = [7.39/11.22, 2.72/11.22, 1.11/11.22]
        = [0.659,       0.242,       0.099]
```

解读：模型认为第 0 类的概率 65.9%，第 1 类 24.2%，第 2 类 9.9%。

### 数值稳定的实现

直接计算 e^z 可能溢出（z 很大时），所以我们用一个技巧：先减去最大值。

```python
def softmax(z):
    """
    Numerically stable softmax.
    z: shape (N, K) — N samples, K classes
    """
    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # subtract max for stability
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)
```

**为什么减最大值不影响结果？** 因为

```math
\frac{e^{(z_i - c)}}{\sum e^{(z_j - c)}} = \frac{e^{z_i} \cdot e^{-c}}{\sum (e^{z_j} \cdot e^{-c})} = \frac{e^{z_i}}{\sum e^{z_j}}
```



常数 $e^{-c}$ 消掉了。

---

## 3.5 完整的前向传播代码

把上面的步骤串起来：

```python
def forward(X, weights, biases):
    """
    Forward pass through a multi-layer network.
    
    X: input, shape (N, 784)
    weights: list of weight matrices [W1, W2, W3]
    biases: list of bias vectors [b1, b2, b3]
    
    Returns: list of activations [X, A1, A2, A3]
    """
    activations = [X]
    num_layers = len(weights)
    
    for i in range(num_layers):
        z = activations[-1] @ weights[i] + biases[i]
        
        if i < num_layers - 1:
            a = np.maximum(0, z)    # ReLU for hidden layers
        else:
            a = softmax(z)          # Softmax for output layer
        
        activations.append(a)
    
    return activations
```

**验证**：

```python
# Initialize
W1 = np.random.randn(784, 64) * np.sqrt(2/784)
b1 = np.zeros((1, 64))
W2 = np.random.randn(64, 32) * np.sqrt(2/64)
b2 = np.zeros((1, 32))
W3 = np.random.randn(32, 10) * np.sqrt(2/32)
b3 = np.zeros((1, 10))

X = np.random.randn(5, 784)    # 5 samples
acts = forward(X, [W1, W2, W3], [b1, b2, b3])

print(acts[-1].shape)            # (5, 10)
print(acts[-1].sum(axis=1))      # [1.0, 1.0, 1.0, 1.0, 1.0] — probabilities!
print(np.argmax(acts[-1], axis=1))  # predicted classes
```

---

## 3.6 从概率到预测

模型输出 10 个概率，我们取最大概率对应的类别作为预测：

```python
predictions = np.argmax(acts[-1], axis=1)
# e.g., [3, 7, 1, 0, 5] — the model's guesses
```

但此时网络还没训练，权重是随机的，所以预测基本是乱猜（大约 10% 准确率，和随机一样）。

如何让网络"学习"？我们需要两样东西：
1. 一种衡量"猜得有多差"的方法 → 损失函数（下一章）
2. 一种"调整权重使猜得更好"的方法 → 反向传播（第 5 章）

---

## 3.7 动手练习

**练习 1**：手动追踪数据的 shape 变化。如果输入是 (32, 784)（32 个样本），经过 784→64→32→10 的网络，每一层输出的 shape 是什么？

**练习 2**：实现 softmax 函数，测试输入 `z = np.array([[1000, 1001, 999]])`，验证不会溢出，且输出之和为 1。

**练习 3**：把前向传播应用到真实的 MNIST 数据。用随机权重计算 100 个测试样本的预测，计算准确率（预计约 10%）。
