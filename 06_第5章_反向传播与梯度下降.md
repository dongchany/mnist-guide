# 第 5 章：让网络学习——反向传播与梯度下降

> **本章目标**：理解梯度下降和反向传播的原理，能手动推导简单情况。
> **预计时间**：3 小时（本教程最核心的一章，值得多花时间）

---

## 5.1 直觉：蒙眼下山

想象你蒙着眼睛站在一座山上，想走到山谷。你的策略：

1. 用脚感受周围哪个方向是**下坡**的（计算梯度）
2. 朝那个方向**走一小步**（更新参数）
3. 重复上述过程

这就是**梯度下降**（Gradient Descent）。

**梯度**告诉你：如果你往某个方向微微移动，损失会增加多少。我们往梯度的**反方向**走，损失就会减小。

---

## 5.2 数学：导数与梯度

### 一维情况

如果损失 L 只取决于一个参数 w：

```
梯度 = dL/dw = lim(ε→0) [L(w+ε) - L(w)] / ε
```

更新规则：
```
w_new = w_old - η · (dL/dw)
```

其中 η（学习率）控制每一步走多远。

### 多维情况

有几万个参数时，梯度是一个**向量**，每个分量告诉你对应参数的导数：

```
∇L = [∂L/∂w₁, ∂L/∂w₂, ..., ∂L/∂wₙ]
```

更新所有参数：
```
W_new = W_old - η · ∂L/∂W
b_new = b_old - η · ∂L/∂b
```

---

## 5.3 链式法则：反向传播的数学基础

问题：损失 L 取决于最后一层输出，最后一层取决于倒数第二层……一直到第一层的权重。怎么求 ∂L/∂W₁？

答案：**链式法则**。

假设有嵌套函数 L = f(g(h(x)))，那么：

```
dL/dx = (dL/df) · (df/dg) · (dg/dh) · (dh/dx)
```

像链条一样，每一环乘在一起。

### 一个简单例子

```
z = w · x + b     (线性)
a = max(0, z)     (ReLU)
L = (a - y)²      (损失)
```

求 ∂L/∂w：

```
∂L/∂w = ∂L/∂a · ∂a/∂z · ∂z/∂w
       = 2(a-y)  · ReLU'(z)  · x
```

从右到左，每一步都很简单。**反向传播**就是系统地从后向前应用链式法则。

---

## 5.4 反向传播：逐层推导

回到我们的三层网络，前向传播是：

```
Z₁ = X @ W₁ + b₁  →  A₁ = ReLU(Z₁)
Z₂ = A₁ @ W₂ + b₂ →  A₂ = ReLU(Z₂)
Z₃ = A₂ @ W₃ + b₃ →  A₃ = Softmax(Z₃)
Loss = CrossEntropy(A₃, Y)
```

反向传播从损失出发，**倒着**一层层算梯度：

### 第 3 层（输出层）

上一章我们说过，交叉熵 + Softmax 的梯度简化为：

```
δ₃ = (A₃ - Y) / N          # shape: (N, 10)
```

这就是"误差信号"——预测与真实的差距。

现在求权重和偏置的梯度：

```
∂L/∂W₃ = A₂ᵀ @ δ₃          # shape: (32, 10)
∂L/∂b₃ = mean(δ₃, axis=0)  # shape: (1, 10)
```

**直觉**：`A₂ᵀ @ δ₃` 的含义是——"第 2 层每个神经元的输出"和"输出层的误差信号"之间的关联。关联越强，对应的权重调整越大。

### 传播误差到第 2 层

```
δ₂_pre = δ₃ @ W₃ᵀ           # 把误差"分配回去", shape: (N, 32)
δ₂ = δ₂_pre * ReLU'(Z₂)    # 乘以激活函数的导数, shape: (N, 32)
```

**直觉**：W₃ᵀ 把误差按照"贡献度"分配给上一层的各个神经元。如果某个权重很大，说明那个神经元"贡献"了更多误差，所以分到更多。

ReLU'(Z₂) 的作用：如果某个神经元在前向传播时被关闭了（Z₂ ≤ 0），梯度为 0，不更新——已经关闭的神经元不"背锅"。

### 第 2 层

```
∂L/∂W₂ = A₁ᵀ @ δ₂          # shape: (64, 32)
∂L/∂b₂ = mean(δ₂, axis=0)  # shape: (1, 32)
```

### 传播误差到第 1 层 + 第 1 层梯度

```
δ₁ = (δ₂ @ W₂ᵀ) * ReLU'(Z₁)   # shape: (N, 64)
∂L/∂W₁ = Xᵀ @ δ₁               # shape: (784, 64)
∂L/∂b₁ = mean(δ₁, axis=0)      # shape: (1, 64)
```

### 梯度汇总

```
对每一层 i（从后向前）：
  δ = 误差信号
  ∂L/∂Wᵢ = 上一层激活ᵀ @ δ
  ∂L/∂bᵢ = mean(δ)
  δ_prev = (δ @ Wᵢᵀ) * ReLU'(Zᵢ₋₁)    （如果还有上一层）
```

---

## 5.5 代码实现

```python
def backward(activations, pre_activations, weights, y_onehot):
    """
    Backpropagation through the network.
    
    activations: [X, A1, A2, A3] from forward pass
    pre_activations: [Z1, Z2, Z3] before activation
    weights: [W1, W2, W3]
    y_onehot: (N, 10) one-hot labels
    
    Returns: (grad_w, grad_b) — lists of gradients for each layer
    """
    num_layers = len(weights)
    m = y_onehot.shape[0]
    grad_w = [None] * num_layers
    grad_b = [None] * num_layers
    
    # Start with output layer gradient (cross-entropy + softmax)
    delta = (activations[-1] - y_onehot) / m
    
    # Work backwards
    for i in reversed(range(num_layers)):
        grad_w[i] = activations[i].T @ delta
        grad_b[i] = np.mean(delta, axis=0, keepdims=True)
        
        if i > 0:
            delta = (delta @ weights[i].T) * (pre_activations[i-1] > 0)
    
    return grad_w, grad_b
```

注意 `(pre_activations[i-1] > 0)` 就是 ReLU 的导数，输出 True/False（即 1/0）。

---

## 5.6 参数更新与小批量 SGD

### 基本梯度下降

```python
for i in range(num_layers):
    weights[i] -= learning_rate * grad_w[i]
    biases[i]  -= learning_rate * grad_b[i]
```

### 为什么用"小批量"？

一次用全部 60,000 个样本算梯度太慢。一次只用 1 个样本又太"抖动"。折中方案：每次用一小批（比如 64 个样本），这叫**小批量随机梯度下降**（Mini-batch SGD）。

### 学习率衰减

训练初期用较大的学习率（快速逼近），后期用较小的学习率（精细调整）：

```python
lr = initial_lr * (decay_rate ** epoch)
# e.g., 0.1 * 0.95^epoch → gradually decreases
```

---

## 5.7 训练循环的完整流程

```python
for epoch in range(num_epochs):
    # Shuffle data
    indices = np.random.permutation(N)
    
    for start in range(0, N, batch_size):
        # Get a mini-batch
        batch_idx = indices[start : start + batch_size]
        X_batch = X_train[batch_idx]
        y_batch = y_onehot[batch_idx]
        
        # Forward pass
        activations, pre_activations = forward(X_batch, weights, biases)
        
        # Compute loss
        loss = cross_entropy(activations[-1], y_batch)
        
        # Backward pass
        grad_w, grad_b = backward(activations, pre_activations, weights, y_batch)
        
        # Update parameters
        for i in range(num_layers):
            weights[i] -= lr * grad_w[i]
            biases[i]  -= lr * grad_b[i]
    
    lr *= decay_rate
```

每个 epoch 遍历一次完整数据集。

---

## 5.8 梯度检验：你写对了吗？

反向传播代码非常容易写错。好消息是，我们可以用**数值梯度**来验证：

```python
def numerical_gradient(loss_fn, param, eps=1e-5):
    """Compute gradient numerically using central difference."""
    grad = np.zeros_like(param)
    it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])
    while not it.finished:
        idx = it.multi_index
        old = param[idx]
        
        param[idx] = old + eps
        loss_plus = loss_fn()
        param[idx] = old - eps
        loss_minus = loss_fn()
        param[idx] = old
        
        grad[idx] = (loss_plus - loss_minus) / (2 * eps)
        it.iternext()
    return grad
```

然后比较数值梯度和你的解析梯度，相对误差应小于 1e-5。这是调试的终极武器，在第 11 章的测试中我们会详细用到。

---

## 5.9 动手练习

**练习 1**：在纸上推导一个 2 输入 → 1 隐藏神经元（ReLU）→ 1 输出（Sigmoid）的网络的反向传播。

**练习 2**：实现完整的训练循环。用随机权重在 MNIST 上训练 1 个 epoch，观察损失是否下降。

**练习 3**：实现数值梯度检验。用一个 4→5→3 的小网络和 3 个样本，比较你的反向传播梯度和数值梯度。
