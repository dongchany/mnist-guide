# 第 6 章：实战——全连接网络识别手写数字

> **本章目标**：把前几章学到的知识整合成完整代码，在 MNIST 上训练并评估。
> **预计时间**：2 小时

---

## 6.1 项目结构

我们将创建以下文件：

```
project/
├── activations.py      ← 共享的激活函数
├── fc_network.py       ← 全连接网络类
├── data_loader.py      ← 数据加载与预处理
└── main.py             ← 训练与评估主脚本
```

---

## 6.2 激活函数模块

把 relu、softmax 等函数集中在一个文件中，避免重复：

```python
# activations.py
import numpy as np

def relu(z):
    return np.maximum(0, z)

def relu_derivative(z):
    return (z > 0).astype(z.dtype)

def softmax(z):
    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

def he_init(fan_in, fan_out):
    return np.random.randn(fan_in, fan_out) * np.sqrt(2.0 / fan_in)

def cross_entropy_loss(probs, y_onehot):
    return -np.mean(np.sum(y_onehot * np.log(np.clip(probs, 1e-12, 1.0)), axis=1))
```

---

## 6.3 全连接网络类

我们用一个类把所有功能封装在一起。跟着注释逐行理解：

```python
# fc_network.py
import numpy as np
from activations import relu, relu_derivative, softmax, he_init, cross_entropy_loss


class FullyConnectedNetwork:
    
    def __init__(self, layer_sizes=(784, 64, 32, 10),
                 learning_rate=0.1, epochs=5, batch_size=64):
        """
        layer_sizes: e.g., [784, 64, 32, 10] means
            784 inputs → 64 hidden → 32 hidden → 10 outputs
        """
        self.layer_sizes = list(layer_sizes)
        self.lr = learning_rate
        self.epochs = epochs
        self.batch_size = batch_size
        self.num_layers = len(layer_sizes) - 1
        
        # Initialize weights with He init, biases with zeros
        self.weights = [he_init(layer_sizes[i], layer_sizes[i+1])
                        for i in range(self.num_layers)]
        self.biases  = [np.zeros((1, layer_sizes[i+1]))
                        for i in range(self.num_layers)]
    
    def _forward(self, X):
        """
        Forward pass.
        Returns: (activations, pre_activations)
            activations[0] = X, activations[-1] = softmax output
        """
        activations = [X]
        pre_activations = []
        
        for i in range(self.num_layers):
            z = activations[-1] @ self.weights[i] + self.biases[i]
            pre_activations.append(z)
            
            # Last layer: softmax; hidden layers: ReLU
            a = softmax(z) if i == self.num_layers - 1 else relu(z)
            activations.append(a)
        
        return activations, pre_activations
    
    def _backward(self, activations, pre_activations, y_onehot):
        """
        Backpropagation.
        Returns: (grad_w, grad_b) lists
        """
        m = y_onehot.shape[0]
        grad_w = [None] * self.num_layers
        grad_b = [None] * self.num_layers
        
        # Output gradient: softmax + cross-entropy simplification
        delta = (activations[-1] - y_onehot) / m
        
        for i in reversed(range(self.num_layers)):
            grad_w[i] = activations[i].T @ delta
            grad_b[i] = np.mean(delta, axis=0, keepdims=True)
            
            if i > 0:  # propagate error to previous layer
                delta = (delta @ self.weights[i].T) * relu_derivative(pre_activations[i-1])
        
        return grad_w, grad_b
    
    def fit(self, X_train, y_train):
        """Train the network."""
        # Convert labels to one-hot
        num_classes = self.layer_sizes[-1]
        y_onehot = np.zeros((len(y_train), num_classes))
        y_onehot[np.arange(len(y_train)), y_train] = 1.0
        
        self.train_losses = []
        lr = self.lr
        n = len(X_train)
        
        for epoch in range(self.epochs):
            # Shuffle data each epoch
            idx = np.random.permutation(n)
            epoch_loss, num_batches = 0.0, 0
            
            for start in range(0, n, self.batch_size):
                end = min(start + self.batch_size, n)
                Xb = X_train[idx[start:end]]
                yb = y_onehot[idx[start:end]]
                
                # Forward → Loss → Backward → Update
                acts, pre_acts = self._forward(Xb)
                epoch_loss += cross_entropy_loss(acts[-1], yb)
                num_batches += 1
                
                gw, gb = self._backward(acts, pre_acts, yb)
                for i in range(self.num_layers):
                    self.weights[i] -= lr * gw[i]
                    self.biases[i]  -= lr * gb[i]
            
            avg_loss = epoch_loss / num_batches
            self.train_losses.append(avg_loss)
            lr *= 0.95  # learning rate decay
            
            # Evaluate on training set
            train_acc = np.mean(self.predict(X_train) == y_train)
            print(f"Epoch {epoch+1}/{self.epochs}  loss={avg_loss:.4f}  train_acc={train_acc:.4f}")
    
    def predict(self, X):
        """Return predicted class labels."""
        probs = self._forward(X)[0][-1]
        return np.argmax(probs, axis=1)
```

---

## 6.4 训练与评估

```python
# main.py
import numpy as np
from data_loader import load_mnist, preprocess_for_fc
from fc_network import FullyConnectedNetwork

# Load data
X_train, y_train, X_test, y_test = load_mnist('./MNIST_data')
X_train_fc, X_test_fc = preprocess_for_fc(X_train, X_test)

# Use 10,000 samples for fast training
np.random.seed(42)
idx = np.random.choice(60000, 10000, replace=False)

# Create and train
model = FullyConnectedNetwork(
    layer_sizes=[784, 64, 32, 10],
    learning_rate=0.1,
    epochs=5,
    batch_size=64
)
model.fit(X_train_fc[idx], y_train[idx])

# Evaluate on test set
test_pred = model.predict(X_test_fc)
test_acc = np.mean(test_pred == y_test)
print(f"\nTest Accuracy: {test_acc:.4f}")
```

预期输出类似：

```
Epoch 1/5  loss=1.2345  train_acc=0.8234
Epoch 2/5  loss=0.5678  train_acc=0.9012
...
Epoch 5/5  loss=0.2345  train_acc=0.9456

Test Accuracy: 0.9300
```

用 10,000 个样本训练 5 个 epoch，通常能达到 **90%+** 的测试准确率。

---

## 6.5 发生了什么？

让我们回顾整个过程：

1. **初始化**：随机权重，模型是"瞎猜的"（约 10% 准确率）
2. **第 1 个 epoch**：模型看了一遍训练数据，每个 batch 都在调整权重，loss 快速下降
3. **后续 epochs**：继续微调，loss 逐渐趋于平缓
4. **测试**：用模型从未见过的数据检验——93% 的图片都能认对！

整个过程中，52,650 个权重参数从随机值变成了有意义的"知识"。

---

## 6.6 观察训练曲线

```python
import matplotlib.pyplot as plt

plt.plot(range(1, 6), model.train_losses, 'b-o')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.show()
```

你应该看到一条下降的曲线——这就是学习的过程。

---

## 6.7 动手练习

**练习 1**：尝试不同的网络结构（比如 [784, 128, 10] 只有一个隐藏层），比较效果。

**练习 2**：调整学习率（试试 0.01 和 1.0），观察训练曲线的变化。太大会怎样？太小呢？

**练习 3**：找出模型预测错误的样本，用 matplotlib 显示出来。观察这些图片——你自己能认出来吗？

**练习 4**：增加训练样本数（试试全部 60,000 个），准确率能提升多少？
