# 第 5 章：让网络学习——反向传播与梯度下降

> **本章目标**：理解梯度下降和反向传播的原理，能手动推导简单情况。
> **预计时间**：3 小时（本教程最核心的一章，值得多花时间）

---

## 5.1 直觉：蒙眼下山

想象你蒙着眼睛站在一座山上，想走到山谷。你的策略：

1. 用脚感受周围哪个方向是**下坡**的（计算梯度）
2. 朝那个方向**走一小步**（更新参数）
3. 重复上述过程

这就是**梯度下降**（Gradient Descent）。

**梯度**告诉你：如果你往某个方向微微移动，损失会增加多少。我们往梯度的**反方向**走，损失就会减小。

---

## 5.2 数学：导数与梯度

### 一维情况

如果损失 $L$ 只取决于一个参数 $w$：

```math
\text{梯度} = \frac{dL}{dw} = \lim_{\varepsilon \to 0} \frac{L(w+\varepsilon) - L(w)}{\varepsilon}
```

更新规则：

```math
w_{\text{new}} = w_{\text{old}} - \eta \cdot \frac{dL}{dw}
```

其中 $\eta$（学习率）控制每一步走多远。

### 多维情况

有几万个参数时，梯度是一个**向量**，每个分量告诉你对应参数的导数：

```math
\nabla L = \left[\frac{\partial L}{\partial w_1}, \frac{\partial L}{\partial w_2}, \ldots, \frac{\partial L}{\partial w_n}\right]
```

更新所有参数：

```math
W_{\text{new}} = W_{\text{old}} - \eta \cdot \frac{\partial L}{\partial W}
```

```math
b_{\text{new}} = b_{\text{old}} - \eta \cdot \frac{\partial L}{\partial b}
```

---

## 5.3 链式法则：反向传播的数学基础

问题：损失 $L$ 取决于最后一层输出，最后一层取决于倒数第二层……一直到第一层的权重。怎么求 $\partial L / \partial W_1$？

答案：**链式法则**。

假设有嵌套函数 $L = f(g(h(x)))$，那么：

```math
\frac{dL}{dx} = \frac{dL}{df} \cdot \frac{df}{dg} \cdot \frac{dg}{dh} \cdot \frac{dh}{dx}
```

像链条一样，每一环乘在一起。

### 一个简单例子

```math
z = w \cdot x + b \quad \text{(线性)}
```

```math
a = \max(0, z) \quad \text{(ReLU)}
```

```math
L = (a - y)^2 \quad \text{(损失)}
```

求 $\partial L / \partial w$：

```math
\frac{\partial L}{\partial w} = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial w} = 2(a-y) \cdot \text{ReLU}'(z) \cdot x
```

从右到左，每一步都很简单。**反向传播**就是系统地从后向前应用链式法则。

---

## 5.4 反向传播：逐层推导

回到我们的三层网络，前向传播是：

```math
Z_1 = X W_1 + b_1 \;\to\; A_1 = \text{ReLU}(Z_1)
```

```math
Z_2 = A_1 W_2 + b_2 \;\to\; A_2 = \text{ReLU}(Z_2)
```

```math
Z_3 = A_2 W_3 + b_3 \;\to\; A_3 = \text{Softmax}(Z_3)
```

```math
\text{Loss} = \text{CrossEntropy}(A_3, Y)
```

反向传播从损失出发，**倒着**一层层算梯度：

### 第 3 层（输出层）

上一章我们说过，交叉熵 + Softmax 的梯度简化为：

```math
\delta_3 = \frac{A_3 - Y}{N} \qquad \text{shape: } (N, 10)
```

这就是"误差信号"——预测与真实的差距。

现在求权重和偏置的梯度：

```math
\frac{\partial L}{\partial W_3} = A_2^\top \delta_3 \qquad \text{shape: } (32, 10)
```

```math
\frac{\partial L}{\partial b_3} = \text{mean}(\delta_3, \text{axis}=0) \qquad \text{shape: } (1, 10)
```

**直觉**：$A_2^\top \delta_3$ 的含义是——"第 2 层每个神经元的输出"和"输出层的误差信号"之间的关联。关联越强，对应的权重调整越大。

### 传播误差到第 2 层

```math
\delta_{2,\text{pre}} = \delta_3 W_3^\top \qquad \text{shape: } (N, 32)
```

```math
\delta_2 = \delta_{2,\text{pre}} \odot \text{ReLU}'(Z_2) \qquad \text{shape: } (N, 32)
```

**直觉**：$W_3^\top$ 把误差按照"贡献度"分配给上一层的各个神经元。如果某个权重很大，说明那个神经元"贡献"了更多误差，所以分到更多。

$\text{ReLU}'(Z_2)$ 的作用：如果某个神经元在前向传播时被关闭了（$Z_2 \leq 0$），梯度为 0，不更新——已经关闭的神经元不"背锅"。

### 第 2 层

```math
\frac{\partial L}{\partial W_2} = A_1^\top \delta_2 \qquad \text{shape: } (64, 32)
```

```math
\frac{\partial L}{\partial b_2} = \text{mean}(\delta_2, \text{axis}=0) \qquad \text{shape: } (1, 32)
```

### 传播误差到第 1 层 + 第 1 层梯度

```math
\delta_1 = (\delta_2 W_2^\top) \odot \text{ReLU}'(Z_1) \qquad \text{shape: } (N, 64)
```

```math
\frac{\partial L}{\partial W_1} = X^\top \delta_1 \qquad \text{shape: } (784, 64)
```

```math
\frac{\partial L}{\partial b_1} = \text{mean}(\delta_1, \text{axis}=0) \qquad \text{shape: } (1, 64)
```

### 梯度汇总

> 对每一层 $i$（从后向前）：
> 
> $\delta$ = 误差信号
> 
> $\partial L / \partial W_i = \text{上一层激活}^\top \cdot \delta$
> 
> $\partial L / \partial b_i = \text{mean}(\delta)$
> 
> $\delta_{\text{prev}} = (\delta \cdot W_i^\top) \odot \text{ReLU}'(Z_{i-1})$（如果还有上一层）

---

## 5.5 代码实现

```python
def backward(activations, pre_activations, weights, y_onehot):
    """
    Backpropagation through the network.
    
    activations: [X, A1, A2, A3] from forward pass
    pre_activations: [Z1, Z2, Z3] before activation
    weights: [W1, W2, W3]
    y_onehot: (N, 10) one-hot labels
    
    Returns: (grad_w, grad_b) — lists of gradients for each layer
    """
    num_layers = len(weights)
    m = y_onehot.shape[0]
    grad_w = [None] * num_layers
    grad_b = [None] * num_layers
    
    # Start with output layer gradient (cross-entropy + softmax)
    delta = (activations[-1] - y_onehot) / m
    
    # Work backwards
    for i in reversed(range(num_layers)):
        grad_w[i] = activations[i].T @ delta
        grad_b[i] = np.mean(delta, axis=0, keepdims=True)
        
        if i > 0:
            delta = (delta @ weights[i].T) * (pre_activations[i-1] > 0)
    
    return grad_w, grad_b
```

注意 `(pre_activations[i-1] > 0)` 就是 ReLU 的导数，输出 True/False（即 1/0）。

---

## 5.6 参数更新与小批量 SGD

### 基本梯度下降

```python
for i in range(num_layers):
    weights[i] -= learning_rate * grad_w[i]
    biases[i]  -= learning_rate * grad_b[i]
```

### 为什么用"小批量"？

一次用全部 60,000 个样本算梯度太慢。一次只用 1 个样本又太"抖动"。折中方案：每次用一小批（比如 64 个样本），这叫**小批量随机梯度下降**（Mini-batch SGD）。

### 学习率衰减

训练初期用较大的学习率（快速逼近），后期用较小的学习率（精细调整）：

```math
\text{lr} = \text{lr}_{\text{init}} \times \text{decay\_rate}^{\text{epoch}}
```

例如 $0.1 \times 0.95^{\text{epoch}}$，逐渐减小。

---

## 5.7 训练循环的完整流程

```python
for epoch in range(num_epochs):
    # Shuffle data
    indices = np.random.permutation(N)
    
    for start in range(0, N, batch_size):
        # Get a mini-batch
        batch_idx = indices[start : start + batch_size]
        X_batch = X_train[batch_idx]
        y_batch = y_onehot[batch_idx]
        
        # Forward pass
        activations, pre_activations = forward(X_batch, weights, biases)
        
        # Compute loss
        loss = cross_entropy(activations[-1], y_batch)
        
        # Backward pass
        grad_w, grad_b = backward(activations, pre_activations, weights, y_batch)
        
        # Update parameters
        for i in range(num_layers):
            weights[i] -= lr * grad_w[i]
            biases[i]  -= lr * grad_b[i]
    
    lr *= decay_rate
```

每个 epoch 遍历一次完整数据集。

---

## 5.8 梯度检验：你写对了吗？

反向传播代码非常容易写错。好消息是，我们可以用**数值梯度**来验证：

```python
def numerical_gradient(loss_fn, param, eps=1e-5):
    """Compute gradient numerically using central difference."""
    grad = np.zeros_like(param)
    it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])
    while not it.finished:
        idx = it.multi_index
        old = param[idx]
        
        param[idx] = old + eps
        loss_plus = loss_fn()
        param[idx] = old - eps
        loss_minus = loss_fn()
        param[idx] = old
        
        grad[idx] = (loss_plus - loss_minus) / (2 * eps)
        it.iternext()
    return grad
```

然后比较数值梯度和你的解析梯度，相对误差应小于 1e-5。这是调试的终极武器，在第 11 章的测试中我们会详细用到。

---

## 5.9 动手练习

**练习 1**：在纸上推导一个 2 输入 → 1 隐藏神经元（ReLU）→ 1 输出（Sigmoid）的网络的反向传播。

**练习 2**：实现完整的训练循环。用随机权重在 MNIST 上训练 1 个 epoch，观察损失是否下降。

**练习 3**：实现数值梯度检验。用一个 4→5→3 的小网络和 3 个样本，比较你的反向传播梯度和数值梯度。
