# 第 10 章：模型可解释性——网络在看什么？

> **本章目标**：理解梯度显著性图原理，学会可视化模型的"注意力"。
> **预计时间**：1.5 小时

---

## 10.1 为什么需要可解释性？

模型说"这是 3"，但它是怎么判断的？它在看笔画的形状，还是碰巧记住了背景噪声？

可解释性帮助我们：
- **发现 bug**：如果模型"看"的地方不对，说明学到了错误的特征
- **建立信任**：知道模型为什么做出某个决定
- **改进模型**：了解模型的弱点在哪里

---

## 10.2 直觉：梯度显著性图

核心问题：**哪些像素对模型的决策影响最大？**

方法很简单：计算模型输出相对于**每个输入像素**的梯度。梯度大的像素 = 微小变化就能大幅影响预测 = 模型很"关注"这个像素。

```
Saliency Map = |∂Score / ∂Input_pixel|
```

取绝对值是因为我们只关心影响的大小，不关心方向。

---

## 10.3 计算方法

对于 FC 网络，我们已经有了反向传播的全部机制。只需要把梯度一直传回输入层：

```python
def compute_gradient_attribution(self, X, target_class=None):
    """Compute saliency map: |dOutput/dInput|."""
    # Forward pass
    activations, pre_activations = self._forward(X)
    probs = activations[-1]
    
    if target_class is None:
        target_class = np.argmax(probs, axis=1)
    
    # Create target: we want the gradient of the correct class score
    y_target = np.zeros_like(probs)
    y_target[np.arange(X.shape[0]), target_class] = 1.0
    
    # Backward pass all the way to input
    delta = probs - y_target
    for i in reversed(range(self.num_layers)):
        if i > 0:
            delta = (delta @ self.weights[i].T) * relu_derivative(pre_activations[i-1])
        else:
            delta = delta @ self.weights[i].T  # gradient w.r.t. input
    
    return np.abs(delta)
```

---

## 10.4 可视化

```python
import matplotlib.pyplot as plt

def show_saliency(model, X_samples, y_samples, is_cnn=False):
    fig, axes = plt.subplots(3, 10, figsize=(20, 6))
    
    for i in range(10):
        x_in = X_samples[i:i+1]
        img = X_samples[i, 0] if is_cnn else X_samples[i].reshape(28, 28)
        
        saliency = model.compute_gradient_attribution(x_in)
        sal_map = saliency[0, 0] if is_cnn else saliency[0].reshape(28, 28)
        
        # Row 0: original image
        axes[0, i].imshow(img, cmap='gray')
        axes[0, i].set_title(f'Label: {y_samples[i]}')
        
        # Row 1: saliency heatmap
        axes[1, i].imshow(sal_map, cmap='hot')
        
        # Row 2: overlay
        axes[2, i].imshow(img, cmap='gray', alpha=0.5)
        axes[2, i].imshow(sal_map, cmap='hot', alpha=0.5)
        
        for row in range(3):
            axes[row, i].axis('off')
    
    plt.tight_layout()
    plt.show()
```

---

## 10.5 解读显著性图

**FC 网络的显著性图**通常比较"散乱"——因为 FC 网络不知道像素的空间关系，所以可能会在意一些看起来无关的位置。

**CNN 的显著性图**更"聚焦"——集中在笔画的边缘和转折处。这是因为卷积核天然关注局部空间特征。

这个对比本身就说明了：**CNN 学到了更有意义的特征表示**。

---

## 10.6 可视化卷积滤波器

除了显著性图，我们还可以直接看 CNN 学到的滤波器长什么样：

```python
filters = cnn_model.conv1_w   # shape: (4, 1, 3, 3)

fig, axes = plt.subplots(1, 4, figsize=(8, 2))
for i in range(4):
    axes[i].imshow(filters[i, 0], cmap='gray')
    axes[i].set_title(f'Filter {i+1}')
    axes[i].axis('off')
plt.suptitle('Learned Conv1 Filters')
plt.show()
```

你会看到类似边缘检测器的模式——水平线、垂直线、对角线等。这些都是网络**自己学到的**，没有人告诉它要学这些。

---

## 10.7 更高级的方法（了解即可）

我们实现的是最基础的"原始梯度"方法。工业界常用的更高级方法包括：

- **Integrated Gradients**：沿从基线到输入的路径积分梯度，解决梯度饱和问题
- **SHAP**：基于博弈论的归因方法，提供理论保证
- **LIME**：用简单模型局部近似复杂模型的决策
- **Grad-CAM**：专用于 CNN，可视化哪些区域对分类重要

这些工具通常由框架提供（如 PyTorch 的 CAPTUM 库）。

---

## 10.8 动手练习

**练习 1**：对你的 FC 和 CNN 模型分别生成显著性图，对比观察差异。

**练习 2**：找一些预测错误的样本，观察它们的显著性图。模型"看"的地方和你预期的一样吗？

**练习 3**：思考题——如果两个模型准确率相同，但一个的显著性图集中在笔画上，另一个散布在背景上，你更信任哪个？为什么？
